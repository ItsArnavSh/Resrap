package resrap

import (
	"math"
	"strings"
	"unicode"
)

type Token struct {
	id   uint32 //Will be generated by the parser
	typ  TokenType
	text string //Generated by the Scanner
}

type TokenType int8

const (
	word        TokenType = iota //Normal words
	character                    //'...'
	maybe                        //?
	oneormore                    //+
	anyno                        //*
	bracks                       //(...)
	option                       // |
	padding                      //just to account for my bad indexing skills
	regexrange                   //[...]
	infinite                     //^
	probability                  //<...>
)

type graphbuilder struct {
	grammar   string
	func_ptr  uint32
	print_ptr uint32
	name_map  map[string]uint32  //Maps func names to their ids
	def_check map[string]bool    //To check if a function exists
	charmap   map[uint32]string  //To store the print values corresponding to ids
	inter_rep map[uint32][]Token //Intermediate Representation
	graph     syntaxGraph        //The final graph
}

func newGraphBuilder() graphbuilder {
	return graphbuilder{
		grammar:   "",
		func_ptr:  0,
		print_ptr: math.MaxInt32,
		name_map:  make(map[string]uint32),
		def_check: make(map[string]bool),
		charmap:   make(map[uint32]string),
		inter_rep: make(map[uint32][]Token),
		graph:     NewSyntaxGraph(),
	}
}
func (g *graphbuilder) start_generation(grammar string) {
	g.grammar = grammar
	g.gen_intermediate_representation()
}
func (g *graphbuilder) gen_intermediate_representation() {

}
func (g *graphbuilder) get_print_ptr() uint32 {
	g.print_ptr--
	return g.print_ptr
}
func (g *graphbuilder) get_func_ptr() uint32 {
	g.func_ptr++
	return g.func_ptr
}
func (g *graphbuilder) SeperateTokens(content string) []Token {
	var tokens []Token
	var buffer strings.Builder

	flush := func() {
		if buffer.Len() > 0 {
			tokens = append(tokens, Token{buffer.String(), word})
			buffer.Reset()
		}
	}

	for i := 0; i < len(content); i++ {
		ch := rune(content[i])

		// Handle // comments
		if ch == '/' && i+1 < len(content) && content[i+1] == '/' {
			flush()
			// Skip until newline
			for i < len(content) && content[i] != '\n' {
				i++
			}
			continue
		}

		switch {
		case unicode.IsSpace(ch):
			flush()
		case ch == '[':
			flush()
			j := i + 1
			depth := 1
			for j < len(content) && depth > 0 {
				if content[j] == '[' {
					depth++
				} else if content[j] == ']' {
					depth--
				}
				j++
			}
			if depth == 0 {
				tokens = append(tokens, Token{content[i:j], regexrange})
				i = j - 1
			} else {
				// unmatched '('
				tokens = append(tokens, Token{string(ch), word})
			}
		case ch == '(':
			flush()
			j := i + 1
			depth := 1
			for j < len(content) && depth > 0 {
				if content[j] == '(' {
					depth++
				} else if content[j] == ')' {
					depth--
				}
				j++
			}
			if depth == 0 {
				tokens = append(tokens, Token{content[i:j], bracks})
				i = j - 1
			} else {
				// unmatched '('
				tokens = append(tokens, Token{string(ch), word})
			}
		case ch == '<':
			flush()
			j := i + 1
			depth := 1
			for j < len(content) && depth > 0 {
				if content[j] == '<' {
					depth++
				} else if content[j] == '>' {
					depth--
				}
				j++
			}
			if depth == 0 {
				tokens = append(tokens, Token{content[i : j-1], probability})
				i = j - 1
			} else {
				// unmatched '<'
				tokens = append(tokens, Token{string(ch), word})
			}
		case ch == '\'':
			flush()
			j := i + 1
			for j < len(content) && rune(content[j]) != '\'' {
				j++
			}
			if j < len(content) {
				tokens = append(tokens, Token{content[i : j+1], character})
				i = j
			} else {
				tokens = append(tokens, Token{string(ch), character})
			}

		case ch == '?':
			flush()
			tokens = append(tokens, Token{"?", maybe})
		case ch == '^':
			flush()
			tokens = append(tokens, Token{"^", infinite})
		case ch == '+':
			flush()
			tokens = append(tokens, Token{"+", oneormore})

		case ch == '*':
			flush()
			tokens = append(tokens, Token{"*", anyno})
		case ch == '|':
			flush()
			tokens = append(tokens, Token{"|", option})
		case ch == ';':
			flush()
			// skip raw semicolon

		default:
			buffer.WriteRune(ch)
		}
	}
	flush()

	return tokens
}
